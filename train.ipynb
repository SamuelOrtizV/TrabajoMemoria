{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARGAR DEPENDENCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from model import *\n",
    "import os\n",
    "import torchvision.transforms.functional as F\n",
    "from inputs.xbox_controller_emulator import XboxControllerEmulator\n",
    "from inputs.GameInputs import reset_environment\n",
    "from inputs.getkeys import key_check\n",
    "from UDP_listener import udp_listener\n",
    "from ScreenRecorder import *\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINICION DE PARAMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Nombre del modelo:\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de captura de pantalla\n",
    "screen_size = (1920, 1080)\n",
    "full_screen = True\n",
    "fps = 100 # HAY QUE MEDIR LA CAPACIDAD Y AJUSTAR ESTE VALOR\n",
    "\n",
    "# Hiperparametros del modelo\n",
    "name = \"SACtest\"\n",
    "architecture = \"CNN\"    # \"CNN\", \"CNN_RNN\", \"CNN_LSTM_STATE\"\n",
    "output_size = 2             # Giro y aceleración\n",
    "\n",
    "# Opción para cargar un modelo entrenado\n",
    "load_model = False                          # Cambia esto a True si deseas cargar un modelo entrenado\n",
    "model_path = \"./trained_models/model.pth\"   # Ruta del modelo entrenado\n",
    "\n",
    "# Hiperparametros de la CNN\n",
    "cnn_name = \"efficientnet_b0\"#\"efficientnet_v2_s\", \"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\"\n",
    "input_size = (240, 135)     # 16:9 ratio\n",
    "dropout = 0.5\n",
    "bias = True                 # Si se desea usar bias en las capas convolucionales\n",
    "cnn_train = True            # Si se desea entrenar la parte CNN\n",
    "\n",
    "# Hiperparametros de la RNN/LSTM\n",
    "hidden_size = 256           # Número de neuronas en la capa oculta de la RNN o LSTM 512 usado por Iker\n",
    "num_layers = 1              # Número de capas en la RNN o LSTM\n",
    "seq_len = 1                 # Número de imágenes a considerar en la secuencia\n",
    "\n",
    "# Hiperparametros de SAC\n",
    "learning_rate = 3e-4   # Tasa de aprendizaje para el optimizador\n",
    "discount_factor = 0.99 # Factor de descuento para las recompensas futurasp\n",
    "alpha = 0.2            # Parámetro de entropía para SAC (controla la exploración)\n",
    "tau = 0.005            # Parámetro de actualización suave de las redes objetivo\n",
    "batch_size = 24      # Tamaño de batch para actualizar el agente\n",
    "\n",
    "# Parámetros de recompensas\n",
    "rewards = {\n",
    "    \"reward_speed_weight\": 0.1,             # Peso de la velocidad en la recompensa\n",
    "    \"reward_track_position_weight\": 5,      # Peso por alcanzar un checkpoint \n",
    "    \"reward_laps_weight\": 500.0,            # Peso de las vueltas completadas DEBE SER ALTO\n",
    "    \"penalty_low_rpms\": -0.2,               # Penalización por quedarse quieto\n",
    "    \"penalty_backwards\": -0.5,              # Penalización por ir hacia atrás\n",
    "    \"penalty_tyres_out\": -0.5,              # Penalización por salirse de la pista\n",
    "    \"penalty_car_damage\": -2.0,\n",
    "    \"threshold_speed\": 10.0,                # Velocidad mínima para recibir recompensa por velocidad\n",
    "    \"threshold_rpms\": 2000.0,               # RPMs mínimas para no recibir recompensa negativa por quedarse quieto\n",
    "    \"threshold_checkpoint\": 0.01            # Umbral de posición en la pista para recibir recompensa por posición\n",
    "    }             # Penalización por dañar el auto\n",
    "\n",
    "# Otras configuraciones\n",
    "max_steps_per_episode = 20   # Máximo número de pasos por episodio\n",
    "num_episodes = 500              # Número de episodios de entrenamiento\n",
    "save_interval = 50              # Guardar el modelo cada 50 episodios\n",
    "dots = [\"   \", \".  \", \".. \", \"...\", \" ..\", \"  .\"]\n",
    "\n",
    "# Inicializar el buffer de experiencia\n",
    "buffer_capacity = 10000\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# Definir el directorio de guardado\n",
    "save_dir = f\"./trained_models/{name}\"\n",
    "\n",
    "# Definir el nombre del modelo a guardar\n",
    "#model_name = f\"{name}-{architecture}-{cnn_name}-{seq_len}-{input_size[0]}-{input_size[1]}-{output_size}-{hidden_size}-ep\"\n",
    "model_name =\"test\"\n",
    "print(f\"\\n\\n\\nNombre del modelo:\\n{model_name}\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True) # Crear directorio de guardado si no existe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCIONES Y CLASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def key_detection():\\n    global stop_event\\n    while not stop_event.is_set():\\n        keys = key_check()\\n        if keys == \"Q\":\\n            stop_event.set()\\n        elif keys == \"P\":\\n            if pause_event.is_set():\\n                pause_event.clear()\\n                print(\"Reanudando el modelo...                                                               \", end=\"\\r\")\\n            else:\\n                print(\"                                                                                            \", end=\"\\r\")\\n                pause_event.set()\\n            time.sleep(1)  # Evitar múltiples detecciones rápidas\\n        elif keys == \"W\":\\n            if output_size == 2:\\n                controller.throttle_break(1.0)\\n                time.sleep(0.5)\\n                controller.reset()\\n\\n# Función para guardar el modelo\\ndef save_model(episode, model_name):\\n    model_save_path = os.path.join(save_dir, model_name+f\"{episode}.pth\")\\n    torch.save({\\n        \\'actor\\': actor.state_dict(),\\n        \\'critic1\\': critic1.state_dict(),\\n        \\'critic2\\': critic2.state_dict(),\\n        \\'target_critic1\\': target_critic1.state_dict(),\\n        \\'target_critic2\\': target_critic2.state_dict(),\\n        \\'actor_optimizer\\': actor_optimizer.state_dict(),\\n        \\'critic1_optimizer\\': critic1_optimizer.state_dict(),\\n        \\'critic2_optimizer\\': critic2_optimizer.state_dict(),\\n    }, model_save_path)\\n    print(f\"Modelo guardado en el episodio {episode}\", end=\"\\r\")\\n\\n# Definir el buffer de experiencia\\nclass ReplayBuffer:\\n    def __init__(self, capacity):\\n        self.buffer = deque(maxlen=capacity)\\n\\n    def push(self, state, action, reward, next_state, done):\\n        self.buffer.append((state, action, reward, next_state, done))\\n\\n    def sample(self, batch_size):\\n        return random.sample(self.buffer, batch_size) # Es una forma de data shuffling\\n\\n    def __len__(self):\\n        return len(self.buffer)\\n\\n# Función para actualizar los modelos\\ndef update_models(batch):\\n    print(\"\\nActualizando modelos...                                                                 \", end=\"\\r\")\\n    states, actions, rewards, next_states, dones = zip(*batch)\\n\\n    # Crear nuevos tensores en lugar de modificar los existentes\\n    states = torch.cat(states).detach()\\n    actions = torch.cat(actions).detach()\\n    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\\n    next_states = torch.cat(next_states).detach()\\n    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\\n\\n    # 1. Actualizar críticos\\n    with torch.no_grad():\\n        next_actions = actor(next_states)\\n        target_q1 = target_critic1(next_states, next_actions)\\n        target_q2 = target_critic2(next_states, next_actions)\\n        target_q = rewards + (discount_factor * torch.min(target_q1, target_q2) * (1 - dones))\\n        target_q = target_q.detach()  # Asegurarnos que target_q está desconectado del grafo\\n\\n    # Actualizar primer crítico\\n    current_q1 = critic1(states, actions)\\n    critic1_loss = F.mse_loss(current_q1, target_q.detach())\\n    critic1_optimizer.zero_grad(set_to_none=True)  # Usar set_to_none=True es más eficiente\\n    critic1_loss.backward(retain_graph=True)\\n    critic1_optimizer.step()\\n\\n    # Actualizar segundo crítico\\n    current_q2 = critic2(states, actions)\\n    critic2_loss = F.mse_loss(current_q2, target_q.detach())\\n    critic2_optimizer.zero_grad(set_to_none=True)\\n    critic2_loss.backward(retain_graph=True)\\n    critic2_optimizer.step()\\n\\n    # 2. Actualizar actor\\n    current_actions = actor(states)\\n    actor_loss = -critic1(states, current_actions).mean()\\n    \\n    actor_optimizer.zero_grad(set_to_none=True)\\n    actor_loss.backward()\\n    actor_optimizer.step()\\n\\n    # 3. Actualizar redes objetivo de forma segura\\n    with torch.no_grad():  # Evitar tracking de gradientes durante la actualización\\n        for target_param, param in zip(target_critic1.parameters(), critic1.parameters()):\\n            new_target_param = (1 - tau) * target_param.data + tau * param.data\\n            target_param.data.copy_(new_target_param)\\n            \\n        for target_param, param in zip(target_critic2.parameters(), critic2.parameters()):\\n            new_target_param = (1 - tau) * target_param.data + tau * param.data\\n            target_param.data.copy_(new_target_param)\\n\\n    # Limpiar la memoria si es necesario\\n    if torch.cuda.is_available():\\n        torch.cuda.empty_cache()\\n\\n# Definir las transformaciones de las imagenes\\ntransform = transforms.Compose([\\n    transforms.Resize((input_size)),  # Cambia el tamaño de las imágenes a (height x width)\\n    transforms.ToTensor(),         # Convierte las imágenes a tensores\\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar la imagen\\n])\\n\\nlatest_image = None\\n\\n# Crear una cola para pasar las imágenes transformadas\\ndef capture_and_transform(region, transform, device, stop_event):\\n    global latest_image\\n    while not stop_event.is_set():\\n        # Capturar la pantalla\\n        start_time = time.time()\\n        img = capture_screen(region)\\n        preprocessed_img = Image.fromarray(img.astype(np.uint8)).convert(\\'RGB\\')\\n        preprocessed_img = transform(preprocessed_img).unsqueeze(0).to(device)\\n        \\n        # Actualizar la última imagen capturada\\n        latest_image = preprocessed_img\\n        \\n        time.sleep(max(0, 1/fps - (time.time() - start_time)))\\n\\n# Iniciar el hilo de captura y transformación de imágenes\\ncapture_thread = threading.Thread(target=capture_and_transform, args=(region, transform, device, stop_event))\\ncapture_thread.start() '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\" # Función para guardar el modelo\n",
    "def save_model(episode, model_name):\n",
    "    model_save_path = os.path.join(save_dir, model_name+f\"{episode}.pth\")\n",
    "    torch.save({\n",
    "        'actor': actor.state_dict(),\n",
    "        'critic1': critic1.state_dict(),\n",
    "        'critic2': critic2.state_dict(),\n",
    "        'target_critic1': target_critic1.state_dict(),\n",
    "        'target_critic2': target_critic2.state_dict(),\n",
    "        'actor_optimizer': actor_optimizer.state_dict(),\n",
    "        'critic1_optimizer': critic1_optimizer.state_dict(),\n",
    "        'critic2_optimizer': critic2_optimizer.state_dict(),\n",
    "    }, model_save_path)\n",
    "    print(f\"Modelo guardado en el episodio {episode}\", end=\"\\r\")\n",
    "\n",
    "# Definir el buffer de experiencia\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size) # Es una forma de data shuffling\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Función para actualizar los modelos\n",
    "def update_models(batch):\n",
    "    print(\"\\nActualizando modelos...                                                                 \", end=\"\\r\")\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    # Crear nuevos tensores en lugar de modificar los existentes\n",
    "    states = torch.cat(states).detach()\n",
    "    actions = torch.cat(actions).detach()\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    next_states = torch.cat(next_states).detach()\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # 1. Actualizar críticos\n",
    "    with torch.no_grad():\n",
    "        next_actions = actor(next_states)\n",
    "        target_q1 = target_critic1(next_states, next_actions)\n",
    "        target_q2 = target_critic2(next_states, next_actions)\n",
    "        target_q = rewards + (discount_factor * torch.min(target_q1, target_q2) * (1 - dones))\n",
    "        target_q = target_q.detach()  # Asegurarnos que target_q está desconectado del grafo\n",
    "\n",
    "    # Actualizar primer crítico\n",
    "    current_q1 = critic1(states, actions)\n",
    "    critic1_loss = F.mse_loss(current_q1, target_q.detach())\n",
    "    critic1_optimizer.zero_grad(set_to_none=True)  # Usar set_to_none=True es más eficiente\n",
    "    critic1_loss.backward(retain_graph=True)\n",
    "    critic1_optimizer.step()\n",
    "\n",
    "    # Actualizar segundo crítico\n",
    "    current_q2 = critic2(states, actions)\n",
    "    critic2_loss = F.mse_loss(current_q2, target_q.detach())\n",
    "    critic2_optimizer.zero_grad(set_to_none=True)\n",
    "    critic2_loss.backward(retain_graph=True)\n",
    "    critic2_optimizer.step()\n",
    "\n",
    "    # 2. Actualizar actor\n",
    "    current_actions = actor(states)\n",
    "    actor_loss = -critic1(states, current_actions).mean()\n",
    "    \n",
    "    actor_optimizer.zero_grad(set_to_none=True)\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    # 3. Actualizar redes objetivo de forma segura\n",
    "    with torch.no_grad():  # Evitar tracking de gradientes durante la actualización\n",
    "        for target_param, param in zip(target_critic1.parameters(), critic1.parameters()):\n",
    "            new_target_param = (1 - tau) * target_param.data + tau * param.data\n",
    "            target_param.data.copy_(new_target_param)\n",
    "            \n",
    "        for target_param, param in zip(target_critic2.parameters(), critic2.parameters()):\n",
    "            new_target_param = (1 - tau) * target_param.data + tau * param.data\n",
    "            target_param.data.copy_(new_target_param)\n",
    "\n",
    "    # Limpiar la memoria si es necesario\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Definir las transformaciones de las imagenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size)),  # Cambia el tamaño de las imágenes a (height x width)\n",
    "    transforms.ToTensor(),         # Convierte las imágenes a tensores\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar la imagen\n",
    "])\n",
    "\n",
    "latest_image = None\n",
    "\n",
    "# Crear una cola para pasar las imágenes transformadas\n",
    "def capture_and_transform(region, transform, device, stop_event):\n",
    "    global latest_image\n",
    "    while not stop_event.is_set():\n",
    "        # Capturar la pantalla\n",
    "        start_time = time.time()\n",
    "        img = capture_screen(region)\n",
    "        preprocessed_img = Image.fromarray(img.astype(np.uint8)).convert('RGB')\n",
    "        preprocessed_img = transform(preprocessed_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Actualizar la última imagen capturada\n",
    "        latest_image = preprocessed_img\n",
    "        \n",
    "        time.sleep(max(0, 1/fps - (time.time() - start_time)))\n",
    "\n",
    "# Iniciar el hilo de captura y transformación de imágenes\n",
    "capture_thread = threading.Thread(target=capture_and_transform, args=(region, transform, device, stop_event))\n",
    "capture_thread.start() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INICIAR MODELO Y CARGAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando emulación del controlador, esperando 1 segundos para evitar lecturas incorrectas...\n",
      "Modelo de controlador cargado.\n",
      "Dispositivo usado: cuda\n",
      "No se cargó ningún modelo. Entrenamiento desde cero.\n"
     ]
    }
   ],
   "source": [
    "def key_detection():\n",
    "    global stop_event\n",
    "    while not stop_event.is_set():\n",
    "        keys = key_check()\n",
    "        if keys == \"Q\":\n",
    "            stop_event.set()\n",
    "        elif keys == \"P\":\n",
    "            if pause_event.is_set():\n",
    "                pause_event.clear()\n",
    "                print(\"Reanudando el modelo...                                                               \", end=\"\\r\")\n",
    "            else:\n",
    "                print(\"                                                                                            \", end=\"\\r\")\n",
    "                pause_event.set()\n",
    "            time.sleep(1)  # Evitar múltiples detecciones rápidas\n",
    "        elif keys == \"W\":\n",
    "            if output_size == 2:\n",
    "                controller.throttle_break(1.0)\n",
    "                time.sleep(0.5)\n",
    "                controller.reset()\n",
    "\n",
    "# Variable global para controlar la interrupción del teclado\n",
    "stop_event = threading.Event()\n",
    "pause_event = threading.Event()\n",
    "\n",
    "# Inicializar el controlador del simulador\n",
    "if output_size == 2:\n",
    "    controller = XboxControllerEmulator()\n",
    "    print(\"\\nModelo de controlador cargado.\")\n",
    "else:\n",
    "    raise ValueError(\"El tamaño de salida del modelo debe ser 2 (control)\")\n",
    "\n",
    "# Definir la región de captura de pantalla\n",
    "region = get_region(screen_size, full_screen)\n",
    "\n",
    "# Activar dispositivo CUDA si está disponible\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Dispositivo usado: {str(device)}\")\n",
    "\n",
    "# Inicializar las redes del actor y crítico\n",
    "actor =             Actor (cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "critic1 =           Critic(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "critic2 =           Critic(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "target_critic1 =    Critic(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "target_critic2 =    Critic(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "\n",
    "# Copiar los pesos de los críticos a los críticos objetivo\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict())\n",
    "\n",
    "# Optimización\n",
    "actor_optimizer =   optim.Adam(actor.parameters()  , lr=learning_rate)\n",
    "critic1_optimizer = optim.Adam(critic1.parameters(), lr=learning_rate)\n",
    "critic2_optimizer = optim.Adam(critic2.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if load_model and os.path.exists(model_path): # Crear una funcion con esto\n",
    "    checkpoint = torch.load(model_path)\n",
    "    actor.load_state_dict(checkpoint['actor'])\n",
    "    critic1.load_state_dict(checkpoint['critic1'])\n",
    "    critic2.load_state_dict(checkpoint['critic2'])\n",
    "    target_critic1.load_state_dict(checkpoint['target_critic1'])\n",
    "    target_critic2.load_state_dict(checkpoint['target_critic2'])\n",
    "    actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "    critic1_optimizer.load_state_dict(checkpoint['critic1_optimizer'])\n",
    "    critic2_optimizer.load_state_dict(checkpoint['critic2_optimizer'])\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se cargó ningún modelo. Entrenamiento desde cero.\")\n",
    "\n",
    "# Iniciar el hilo de detección de teclas\n",
    "key_thread = threading.Thread(target=key_detection)\n",
    "key_thread.start()\n",
    "\n",
    "# pause_event.set() # Pausar el modelo al inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para guardar el modelo\n",
    "def save_model(episode, model_name):\n",
    "    model_save_path = os.path.join(save_dir, model_name+f\"{episode}.pth\")\n",
    "    torch.save({\n",
    "        'actor': actor.state_dict(),\n",
    "        'critic1': critic1.state_dict(),\n",
    "        'critic2': critic2.state_dict(),\n",
    "        'target_critic1': target_critic1.state_dict(),\n",
    "        'target_critic2': target_critic2.state_dict(),\n",
    "        'actor_optimizer': actor_optimizer.state_dict(),\n",
    "        'critic1_optimizer': critic1_optimizer.state_dict(),\n",
    "        'critic2_optimizer': critic2_optimizer.state_dict(),\n",
    "    }, model_save_path)\n",
    "    print(f\"Modelo guardado en el episodio {episode}\", end=\"\\r\")\n",
    "\n",
    "# Definir el buffer de experiencia\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size) # Es una forma de data shuffling\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Función para actualizar los modelos\n",
    "def update_models(batch):\n",
    "    print(\"\\nActualizando modelos...                                                                 \", end=\"\\r\")\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    # Crear nuevos tensores en lugar de modificar los existentes\n",
    "    states = torch.cat(states).detach()\n",
    "    actions = torch.cat(actions).detach()\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    next_states = torch.cat(next_states).detach()\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # 1. Actualizar críticos\n",
    "    with torch.no_grad():\n",
    "        next_actions = actor(next_states)\n",
    "        target_q1 = target_critic1(next_states, next_actions)\n",
    "        target_q2 = target_critic2(next_states, next_actions)\n",
    "        target_q = rewards + (discount_factor * torch.min(target_q1, target_q2) * (1 - dones))\n",
    "        target_q = target_q.detach()  # Asegurarnos que target_q está desconectado del grafo\n",
    "\n",
    "    # Actualizar primer crítico\n",
    "    current_q1 = critic1(states, actions)\n",
    "    critic1_loss = F.mse_loss(current_q1, target_q.detach())\n",
    "    critic1_optimizer.zero_grad(set_to_none=True)  # Usar set_to_none=True es más eficiente\n",
    "    critic1_loss.backward(retain_graph=True)\n",
    "    critic1_optimizer.step()\n",
    "\n",
    "    # Actualizar segundo crítico\n",
    "    current_q2 = critic2(states, actions)\n",
    "    critic2_loss = F.mse_loss(current_q2, target_q.detach())\n",
    "    critic2_optimizer.zero_grad(set_to_none=True)\n",
    "    critic2_loss.backward(retain_graph=True)\n",
    "    critic2_optimizer.step()\n",
    "\n",
    "    # 2. Actualizar actor\n",
    "    current_actions = actor(states)\n",
    "    actor_loss = -critic1(states, current_actions).mean()\n",
    "    \n",
    "    actor_optimizer.zero_grad(set_to_none=True)\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    # 3. Actualizar redes objetivo de forma segura\n",
    "    with torch.no_grad():  # Evitar tracking de gradientes durante la actualización\n",
    "        for target_param, param in zip(target_critic1.parameters(), critic1.parameters()):\n",
    "            new_target_param = (1 - tau) * target_param.data + tau * param.data\n",
    "            target_param.data.copy_(new_target_param)\n",
    "            \n",
    "        for target_param, param in zip(target_critic2.parameters(), critic2.parameters()):\n",
    "            new_target_param = (1 - tau) * target_param.data + tau * param.data\n",
    "            target_param.data.copy_(new_target_param)\n",
    "\n",
    "    # Limpiar la memoria si es necesario\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Definir las transformaciones de las imagenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size)),  # Cambia el tamaño de las imágenes a (height x width)\n",
    "    transforms.ToTensor(),         # Convierte las imágenes a tensores\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar la imagen\n",
    "])\n",
    "\n",
    "latest_image = None\n",
    "\n",
    "# Crear una cola para pasar las imágenes transformadas\n",
    "def capture_and_transform(region, transform, device, stop_event):\n",
    "    global latest_image\n",
    "    while not stop_event.is_set():\n",
    "        # Capturar la pantalla\n",
    "        start_time = time.time()\n",
    "        img = capture_screen(region)\n",
    "        preprocessed_img = Image.fromarray(img.astype(np.uint8)).convert('RGB')\n",
    "        preprocessed_img = transform(preprocessed_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Actualizar la última imagen capturada\n",
    "        latest_image = preprocessed_img\n",
    "        \n",
    "        time.sleep(max(0, 1/fps - (time.time() - start_time)))\n",
    "\n",
    "# Iniciar el hilo de captura y transformación de imágenes\n",
    "capture_thread = threading.Thread(target=capture_and_transform, args=(region, transform, device, stop_event))\n",
    "capture_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n",
      "\n",
      "Entrenamiento terminado.                                                            \n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando entrenamiento...\")\n",
    "\n",
    "episodio = 0\n",
    "\n",
    "start_time = time.time()    # Tiempo de inicio del entrenamiento\n",
    "\n",
    "# Iniciar el entrenamiento\n",
    "try:\n",
    "    while not stop_event.is_set(): #Ciclos de episodios\n",
    "\n",
    "        if pause_event.is_set():\n",
    "            controller.reset()\n",
    "            print(\"Modelo pausado. Presione 'P' para reanudar.                                                                  \", end=\"\\r\")\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        episodio += 1\n",
    "\n",
    "        tiempos = []\n",
    "        demoras = []\n",
    "\n",
    "        # Reiniciar el entorno\n",
    "        # reset_environment()\n",
    "        previous_location = {\n",
    "            \"previous_checkpoint\": 0.0,\n",
    "            \"previous_position\": 0.0,\n",
    "            \"previous_lap\": 0\n",
    "        }        \n",
    "\n",
    "        # time.sleep(1)  # Esperar un segundo para que el entorno se reinicie completamente\n",
    "        episode_start_time = time.time()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while (not stop_event.is_set()) and (steps < max_steps_per_episode): #Ciclos de pasos\n",
    "            \n",
    "            step_start_time = time.time()\n",
    "\n",
    "            if pause_event.is_set():\n",
    "                controller.reset()\n",
    "                print(\"Modelo pausado. Presione 'P' para reanudar.                                                                  \", end=\"\\r\")\n",
    "                time.sleep(0.1)\n",
    "                continue\n",
    "\n",
    "            # Capturar la pantalla  (Esta parte es muy CPU demandante, sobre todo aplicar las transformaciones) \n",
    "\n",
    "            \"\"\" img = capture_screen(region)          \n",
    "            \n",
    "            preprocessed_img = Image.fromarray(img.astype(np.uint8)).convert('RGB')# Convertir la imagen preprocesada a un objeto PIL y aplicar las transformaciones\n",
    "            state = transform(preprocessed_img).unsqueeze(0).to(device)\n",
    "            \"\"\"      \n",
    "\n",
    "            moving_dots = dots[steps % len(dots)]\n",
    "\n",
    "            # Obtener el estado actual del entorno\n",
    "            if latest_image is not None:\n",
    "                # Procesar la última imagen capturada\n",
    "                state = latest_image\n",
    "            else:\n",
    "                time.sleep(0.01)  # Esperar un poco si la cola está vacía\n",
    "                continue\n",
    "\n",
    "            # Habilitar precisión mixta y para usar la GPU para la inferencia\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                                     \n",
    "\n",
    "                # Elegir acción basada en las características extraídas por la CNN\n",
    "                action = actor(state)  # El actor toma el estado como entrada y produce una acción\n",
    "\n",
    "                # Enviar la acción al entorno\n",
    "                prediction = torch.clamp(action, min=-1.0, max=1.0).tolist()[0]  # Limitar los valores de la acción entre -1.0 y 1.0 y convertir a lista\n",
    "                controller.steering(prediction[0])  # Enviar la acción de dirección al simulador\n",
    "                controller.throttle_break(prediction[1])  # Enviar la acción de aceleración/freno al simulador\n",
    "\n",
    "                # Obtener el siguiente estado del entorno\n",
    "                img = capture_screen(region)\n",
    "                preprocessed_img = Image.fromarray(img.astype(np.uint8)).convert('RGB')# Convertir la imagen preprocesada a un objeto PIL y aplicar las transformaciones\n",
    "                preprocessed_img = transform(preprocessed_img).unsqueeze(0).to(device)\n",
    "                next_state = preprocessed_img \n",
    "\n",
    "                # Aqui puede ser necesario un sleep, probar primero...\n",
    "                next_state = latest_image  # Obtener el siguiente estado del entorno\n",
    "\n",
    "                # Telemetria del juego\n",
    "\n",
    "                # variables = udp_listener() #Causa cuello de botella solo cuando no está recibiendo datos\n",
    "\n",
    "                variables = { # Placeholder para las variables del entorno\n",
    "                \"speed\": 0.0,\n",
    "                \"rpms\": 0,\n",
    "                \"laps\": 0,\n",
    "                \"track_position\": 0.0,\n",
    "                \"tyres_out\": 0,\n",
    "                \"car_damage\": 0.0,\n",
    "                \"transmitting\": False\n",
    "                }\n",
    "\n",
    "                \"\"\" if variables[\"transmitting\"] == False: # Si no se reciben datos de telemetría, continuar con el siguiente paso\n",
    "                    print(\"No se están recibiendo datos de telemetría, esperando...                                                   \", end=\"\\r\")\n",
    "                    continue  \"\"\"\n",
    "\n",
    "                # Calcular la recompensa\n",
    "                reward, done = calculate_reward(variables, rewards, previous_location)  # Se calcula la recompensa basada en las variables del entorno\n",
    "\n",
    "                # Almacenar la transición en el buffer de experiencia\n",
    "                replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "                # Muestrear un batch del buffer y actualizar los modelos\n",
    "                if len(replay_buffer) > batch_size:\n",
    "                    #with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                        batch = replay_buffer.sample(batch_size)\n",
    "                        update_models(batch) \n",
    "\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            next_state = latest_image\n",
    "            flag = False\n",
    "            if torch.equal(state, next_state):\n",
    "                flag = True\n",
    "\n",
    "            # Condición para reiniciar el episodio si el auto está fuera de la pista o dañado\n",
    "            \"\"\" if done:\n",
    "                break \"\"\"  # Termina el episodio si el auto está fuera de la pista o dañado           \n",
    "\n",
    "            #print(f\"{moving_dots} Episodio: {episodio} Recomensa acumulada: {total_reward:.2f} Predicción del modelo: Steering {prediction[0]:.2f} Throttle {prediction[1]:.2f} Duración: {step_time:.2f}\", end=\"\\r\")\n",
    "            # Esperar para mantener los FPS\n",
    "            time.sleep(max(0, 1/fps - (time.time() - step_start_time)))\n",
    "\n",
    "            step_time = time.time() - step_start_time\n",
    "            acumulated_time = time.time() - episode_start_time\n",
    "            print(f\"Episodio: {episodio} FPS promedio: {int(steps/acumulated_time)} Duración: {step_time:.4f} FPS: {int(1/step_time)} Rep: {flag}               \", end=\"\\r\")\n",
    "\n",
    "        # Guardar el modelo cada cierto número de episodios\n",
    "        \"\"\" if episodio % save_interval == 0:\n",
    "            save_model(episodio, model_name) \"\"\"                      \n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nEntrenamiento interrumpido.                                                   \")\n",
    "    \"\"\" except Exception as e:\n",
    "    print(f\"\\nError: {e}\") \"\"\"\n",
    "finally:\n",
    "    print(\"\\nEntrenamiento terminado.                                                            \")\n",
    "    stop_event.set()\n",
    "    # Limpiar y cerrar\n",
    "    controller.reset()\n",
    "    key_thread.join()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
