{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARGAR DEPENDENCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from model import *\n",
    "import os\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms, models\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINICION DE PARAMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNminitest-CNN_RNN-efficientnet_b0-5-240-135-2-256-epoch\n"
     ]
    }
   ],
   "source": [
    "name = \"CNNminitest\"\n",
    "architecture = \"CNN_RNN\" # \"CNN\", \"CNN_RNN\", \"CNN_LSTM_STATE\"\n",
    "cnn_name = \"efficientnet_b0\"#\"efficientnet_v2_s\", \"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\"\n",
    "hidden_size = 256 # Número de neuronas en la capa oculta 512 usado por Iker\n",
    "output_size = 2 # Giro y aceleración\n",
    "input_size = (240, 135)  # 16:9 ratio\n",
    "num_layers = 1\n",
    "dropout = 0 # Se necesita num_layers > 1 para que el dropout tenga efecto\n",
    "bias = True\n",
    "cnn_train = True # Si se desea entrenar la parte CNN \n",
    "\n",
    "seq_len = 5 # Número de imágenes a considerar en la secuencia\n",
    "batch_size = 32 # Número de secuencias a considerar en paralelo\n",
    "num_epochs = 30 # Número de veces que se recorrerá el dataset\n",
    "learning_rate = 0.001 \n",
    "\n",
    "# Parámetros del early stopping\n",
    "patience = 5  # Tolerancia de 5 epochs sin mejora\n",
    "best_val_loss = float('inf')  # Inicia con el peor valor posible\n",
    "early_stop_counter = 0  # Contador de epochs sin mejoras\n",
    "\n",
    "# Definir las rutas de los directorios de datos y de guardado de modelos\n",
    "train_data_dir = \"./datasets/test\"\n",
    "validation_data_dir = \"./datasets/test3\"\n",
    "save_dir = f\"./trained_models/{name}\"\n",
    "\n",
    "# Definir el nombre del modelo a guardar\n",
    "model_name = f\"{name}-{architecture}-{cnn_name}-{seq_len}-{input_size[0]}-{input_size[1]}-{output_size}-{hidden_size}-epoch\"\n",
    "print(f\"{model_name}\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True) # Crear directorio de guardado si no existe\n",
    "\n",
    "# Definir el escritor de TensorBoard para visualización\n",
    "writer = SummaryWriter(log_dir=\"./runs/\" + model_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INICIAR MODELO Y CARGAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando CUDA\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el modelo\n",
    "\n",
    "if architecture == \"CNN\":\n",
    "    model = CNN(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train)\n",
    "    seq_len = 1\n",
    "elif architecture == \"CNN_RNN\":\n",
    "    model = CNN_RNN(cnn_name, hidden_size, output_size, (3, *input_size), num_layers, dropout, bias)\n",
    "elif architecture == \"CNN_LSTM_STATE\":\n",
    "    model = CNN_LSTM_STATE(cnn_name, hidden_size, output_size, (3, *input_size), num_layers, dropout, bias)\n",
    "    hidden_state = model.init_hidden(batch_size)\n",
    "\n",
    "\n",
    "# Cargar el modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Usando CUDA\" if torch.cuda.is_available() else \"USANDO CPU\")\n",
    "\n",
    "\n",
    "# Cargar los datasets\n",
    "train_dataset = RacingDataset(data_dir= train_data_dir, seq_len= seq_len, input_size=input_size, controller = True)\n",
    "test_dataset = RacingDataset(data_dir= validation_data_dir, seq_len= seq_len, input_size=input_size, controller = True)\n",
    "\n",
    "# Crear los dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Optimización y función de pérdida\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#criterion = CrossEntropyLoss(weight=torch.tensor(weights).to(device))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n",
      "Trabajando en Epoch 1. Progreso: 100.00%\n",
      "Validando modelo...\n",
      "Validando en Epoch 1. Progreso: 100.00%\n",
      "Epoch [1/30], Train Loss: 0.1852, Val Loss: 0.1454, Epoch Time: 00:01:04, Total Time: 00:01:04\n",
      "Trabajando en Epoch 2. Progreso: 100.00%\n",
      "Validando modelo...\n",
      "Validando en Epoch 2. Progreso: 100.00%\n",
      "Epoch [2/30], Train Loss: 0.0749, Val Loss: 0.1432, Epoch Time: 00:01:05, Total Time: 00:02:09\n",
      "Trabajando en Epoch 3. Progreso: 100.00%\n",
      "Validando modelo...\n",
      "Validando en Epoch 3. Progreso: 100.00%\n",
      "Epoch [3/30], Train Loss: 0.0592, Val Loss: 0.1219, Epoch Time: 00:01:05, Total Time: 00:03:14\n",
      "Trabajando en Epoch 4. Progreso: 6.45%\n",
      "Entrenamiento interrumpido.\n",
      "Entrenamiento terminado.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando entrenamiento...\")\n",
    "\n",
    "start_time = time.time()    # Tiempo de inicio del entrenamiento\n",
    "\n",
    "# Ciclo de entrenamiento\n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "    epoch_start_time = time.time()  # Tiempo de inicio del epoch    \n",
    "\n",
    "    # Inicializar el estado oculto (hidden_state) y cell_state\n",
    "    if architecture == \"CNN_LSTM_STATE\":\n",
    "        hidden_state = model.init_hidden(batch_size)  # Estado inicial para LSTM\n",
    "\n",
    "    model.train() # Establecer el modo de entrenamiento\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    try:\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            print(f\"Trabajando en Epoch {epoch+1}. Progreso: {(i+1)/len(train_loader)*100:.2f}%\", end='\\r')\n",
    "\n",
    "            #print(f\"Dimensiones de las etiquetas en el batch {i+1}: {labels.size()}\")\n",
    "\n",
    "            input_sequence = images # (seq_len, batch_size, channels, height, width)\n",
    "            input_sequence, labels = input_sequence.to(device), labels.to(device) # Mover los datos al dispositivo            \n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Habilitar precisión mixta\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "\n",
    "                if architecture == \"CNN_LSTM_STATE\":\n",
    "                    # Verificar si el tamaño del hidden_state es compatible con el tamaño del lote actual\n",
    "                    current_batch_size = input_sequence.size(0)\n",
    "                    if hidden_state[0].size(1) != current_batch_size:\n",
    "                        hidden_state = model.init_hidden(current_batch_size)\n",
    "                        hidden_state = (hidden_state[0].to(device), hidden_state[1].to(device))\n",
    "                        \n",
    "                    outputs, hidden_state = model(input_sequence, hidden_state)  # Pasar hidden_state\n",
    "                    # Desconectar hidden_state para evitar acumulación de gradientes\n",
    "                    hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "                elif architecture == \"CNN_RNN\":\n",
    "                    outputs = model(input_sequence)\n",
    "                elif architecture == \"CNN\":\n",
    "                    outputs = model(input_sequence)\n",
    "                else:\n",
    "                    print(\"Arquitectura no soportada\")\n",
    "                    #parar la ejecucion de todo el programa\n",
    "                    raise SystemExit\n",
    "\n",
    "                loss = criterion(outputs, labels) # Solo se considera la última etiqueta de la secuencia\n",
    "            \n",
    "            # Backpropagation y optimización\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Registrar la pérdida en TensorBoard\n",
    "        writer.add_scalar('Loss/train', running_loss / len(train_loader), epoch)\n",
    "\n",
    "        # Guardar el modelo después de cada epoch\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'{model_name}_{epoch+1}.pth'))\n",
    "\n",
    "        print(\"\\nValidando modelo...\")\n",
    "\n",
    "        # Validación en el conjunto de datos de prueba\n",
    "        model.eval()  # Establecer el modo de evaluación\n",
    "        test_loss = 0.0\n",
    "\n",
    "        # Inicializar el hidden_state para el set de validación\n",
    "        if architecture == \"CNN_LSTM_STATE\":\n",
    "            hidden_state_val = model.init_hidden(batch_size)\n",
    "            #hidden_state_val = (hidden_state_val[0].to(device), hidden_state_val[1].to(device))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(validation_loader):\n",
    "                \n",
    "                print(f\"Validando en Epoch {epoch+1}. Progreso: {(i+1)/len(validation_loader)*100:.2f}%\", end='\\r')\n",
    "\n",
    "                input_test_sequence = images # (seq_len, batch_size, channels, height, width)\n",
    "                input_test_sequence, labels = input_test_sequence.to(device), labels.to(device) # Mover los datos al dispositivo\n",
    "\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    if architecture == \"CNN_LSTM_STATE\":\n",
    "                        current_batch_size = input_test_sequence.size(0)\n",
    "                        # Actualizar el tamaño del hidden_state_val según el tamaño del lote actual\n",
    "                        if hidden_state_val[0].size(1) != current_batch_size:                     \n",
    "                            hidden_state_val = model.init_hidden(current_batch_size)\n",
    "                            hidden_state_val = (hidden_state_val[0].to(device), hidden_state_val[1].to(device))    \n",
    "\n",
    "                        outputs, hidden_state_val = model(input_test_sequence, hidden_state_val)\n",
    "                        # Desconectar hidden_state_val para evitar acumulación de gradientes\n",
    "                        hidden_state_val = (hidden_state_val[0].detach(), hidden_state_val[1].detach())\n",
    "                    elif architecture == \"CNN\":\n",
    "                        outputs = model(input_test_sequence)\n",
    "                    elif architecture == \"CNN_RNN\":\n",
    "                        outputs = model(input_test_sequence)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "        val_loss = test_loss / len(validation_loader)\n",
    "        writer.add_scalar('Loss/test', val_loss, epoch)\n",
    "\n",
    "        # Comprobar si early stopping es necesario\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0  # Restablecer el contador\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f'{model_name}_{epoch+1}.pth'))  # Guardar el mejor modelo\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "        end_time = time.time() - start_time  # Tiempo de finalización del epoch\n",
    "        # Convertir end_time a formato hh:mm:ss\n",
    "        end_time = time.strftime(\"%H:%M:%S\", time.gmtime(end_time))\n",
    "        epoch_time = time.time() - epoch_start_time  # Tiempo de finalización del epoch\n",
    "        epoch_time = time.strftime(\"%H:%M:%S\", time.gmtime(epoch_time))\n",
    "        print(f'\\nEpoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "            f'Val Loss: {test_loss/len(validation_loader):.4f}, '\n",
    "            f'Epoch Time: {epoch_time}, '\n",
    "            f'Total Time: {end_time}'\n",
    "            )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nEntrenamiento interrumpido.\")\n",
    "        break\n",
    "print(\"Entrenamiento terminado.\")\n",
    "\n",
    "writer.close()  # Cerrar el escritor de TensorBoard\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
