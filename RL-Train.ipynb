{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARGAR DEPENDENCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from model import *\n",
    "import os\n",
    "import torchvision.transforms.functional as F\n",
    "from inputs.xbox_controller_emulator import XboxControllerEmulator\n",
    "from inputs.GameInputs import reset_environment\n",
    "from inputs.getkeys import key_check\n",
    "from UDP_listener import udp_listener\n",
    "from ScreenRecorder import *\n",
    "from torchvision import transforms, models\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINICION DE HIPERPÁRAMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SACtest-CNN-efficientnet_b0-1-240-135-2-256-ep\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de captura de pantalla\n",
    "screen_size = (1920, 1080)\n",
    "full_screen = True\n",
    "fps = 5 # HAY QUE MEDIR LA CAPACIDAD Y AJUSTAR ESTE VALOR\n",
    "\n",
    "# Hiperparametros del modelo\n",
    "name = \"SACtest\"\n",
    "architecture = \"CNN\"    # \"CNN\", \"CNN_RNN\", \"CNN_LSTM_STATE\"\n",
    "output_size = 2             # Giro y aceleración\n",
    "\n",
    "# Opción para cargar un modelo entrenado\n",
    "load_model = False                          # Cambia esto a True si deseas cargar un modelo entrenado\n",
    "model_path = \"./trained_models/model.pth\"   # Ruta del modelo entrenado\n",
    "\n",
    "# Hiperparametros de la CNN\n",
    "cnn_name = \"efficientnet_b0\"#\"efficientnet_v2_s\", \"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\"\n",
    "input_size = (240, 135)     # 16:9 ratio\n",
    "dropout = 0.5               # Dropout para regularización\n",
    "bias = True                 # Si se desea usar bias en las capas convolucionales\n",
    "cnn_train = True            # Si se desea entrenar la parte CNN\n",
    "\n",
    "# Hiperparametros de la RNN/LSTM\n",
    "hidden_size = 256           # Número de neuronas en la capa oculta de la RNN o LSTM 512 usado por Iker\n",
    "num_layers = 1              # Número de capas en la RNN o LSTM\n",
    "seq_len = 1                 # Número de imágenes a considerar en la secuencia\n",
    "\n",
    "# Hiperparametros de SAC\n",
    "learning_rate = 3e-4   # Tasa de aprendizaje para el optimizador\n",
    "discount_factor = 0.99 # Factor de descuento para las recompensas futuras\n",
    "alpha = 0.2            # Parámetro de entropía para SAC (controla la exploración)\n",
    "tau = 0.005            # Parámetro de actualización suave de las redes objetivo\n",
    "batch_size = 64        # Tamaño de batch para actualizar el agente\n",
    "\n",
    "# Parámetros de recompensas\n",
    "reward_speed_weight = 0.1           # Peso de la velocidad en la recompensa\n",
    "reward_track_position_weight = 10.0  # Peso de la posición en la pista\n",
    "reward_laps_weight = 100.0          # Peso de las vueltas completadas DEBE SER ALTO\n",
    "penalty_tyres_out = -0.5            # Penalización por salirse de la pista\n",
    "penalty_car_damage = -2.0           # Penalización por dañar el auto\n",
    "\n",
    "# Otras configuraciones\n",
    "max_steps_per_episode = 1000    # Máximo número de pasos por episodio\n",
    "num_episodes = 500              # Número de episodios de entrenamiento\n",
    "save_interval = 50              # Guardar el modelo cada 50 episodios\n",
    "\n",
    "# Definir el directorio de guardado\n",
    "save_dir = f\"./trained_models/{name}\"\n",
    "\n",
    "# Definir el nombre del modelo a guardar\n",
    "model_name = f\"{name}-{architecture}-{cnn_name}-{seq_len}-{input_size[0]}-{input_size[1]}-{output_size}-{hidden_size}-ep\"\n",
    "print(f\"{model_name}\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True) # Crear directorio de guardado si no existe\n",
    "\n",
    "# Definir el escritor de TensorBoard para visualización\n",
    "writer = SummaryWriter(log_dir=\"./runs/\" + model_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCIONES AUXILIARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable global para controlar la interrupción del teclado\n",
    "stop_event = threading.Event()\n",
    "pause_event = threading.Event()\n",
    "\n",
    "def key_detection():\n",
    "    global stop_event\n",
    "    while not stop_event.is_set():\n",
    "        keys = key_check()\n",
    "        if keys == \"Q\":\n",
    "            stop_event.set()\n",
    "        elif keys == \"P\":\n",
    "            if pause_event.is_set():\n",
    "                pause_event.clear()\n",
    "                print(\"Reanudando el modelo...\", end=\"\\r\")\n",
    "            else:\n",
    "                print(\"                                                                                            \", end=\"\\r\")\n",
    "                pause_event.set()\n",
    "            time.sleep(1)  # Evitar múltiples detecciones rápidas\n",
    "        elif keys == \"W\":\n",
    "            if output_size == 2:\n",
    "                controller.throttle_break(1.0)\n",
    "                time.sleep(0.5)\n",
    "                controller.reset()\n",
    "\n",
    "# Función para guardar el modelo\n",
    "def save_model(episode, model_name):\n",
    "    model_save_path = os.path.join(save_dir, model_name+f\"{episode}.pth\")\n",
    "    torch.save({\n",
    "        'actor': actor.state_dict(),\n",
    "        'critic1': critic1.state_dict(),\n",
    "        'critic2': critic2.state_dict(),\n",
    "        'target_critic1': target_critic1.state_dict(),\n",
    "        'target_critic2': target_critic2.state_dict(),\n",
    "        'actor_optimizer': actor_optimizer.state_dict(),\n",
    "        'critic1_optimizer': critic1_optimizer.state_dict(),\n",
    "        'critic2_optimizer': critic2_optimizer.state_dict(),\n",
    "    }, model_save_path)\n",
    "    print(f\"Modelo guardado en el episodio {episode}\", end=\"\\r\")\n",
    "\n",
    "# Función para calcular la recompensa en cada paso\n",
    "def calculate_reward(variables):\n",
    "    \"\"\"Calcula la recompensa basándose en las variables del entorno\"\"\"\n",
    "\n",
    "    speed = variables[\"speed\"]\n",
    "    track_position = variables[\"track_position\"]\n",
    "    laps = variables[\"laps\"]\n",
    "    tyres_out = variables[\"tyres_out\"]\n",
    "    car_damage = variables[\"car_damage\"]\n",
    "\n",
    "    speed_threshold = 10.0  # Velocidad mínima en km/h para recibir recompensa por velocidad no nula\n",
    "    \n",
    "    if speed < speed_threshold:\n",
    "        reward_speed_weight = -reward_speed_weight  # No recompensar por velocidad si es menor al umbral\n",
    "\n",
    "    # Recompensa por velocidad y posición en la pista\n",
    "    reward = reward_speed_weight + reward_track_position_weight * track_position + reward_laps_weight * laps\n",
    "\n",
    "    # Penalización por salirse de la pista y por daño\n",
    "    if tyres_out > 0:\n",
    "        reward += penalty_tyres_out * tyres_out\n",
    "    if car_damage > 0:\n",
    "        reward += penalty_car_damage\n",
    "\n",
    "    return reward\n",
    "\n",
    "# Definir las transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size)),  # Cambia el tamaño de las imágenes a (height x width)\n",
    "    transforms.ToTensor(),         # Convierte las imágenes a tensores\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar la imagen\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INICIAR MODELO Y CARGAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando CUDA\n",
      "No se cargó ningún modelo. Entrenamiento desde cero.\n",
      "Modelo de controlador cargado.ntrolador, esperando 1 segundos para evitar lecturas incorrectas...\n"
     ]
    }
   ],
   "source": [
    "# Definir la región de captura de pantalla\n",
    "region = get_region(screen_size, full_screen)\n",
    "\n",
    "# Activar dispositivo CUDA si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando CUDA\" if torch.cuda.is_available() else \"USANDO CPU\")\n",
    "\n",
    "# Inicializar las redes del actor y crítico\n",
    "actor =             Actor (cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "critic1 =           Critic(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "critic2 =           Critic(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "target_critic1 =    Critic(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "target_critic2 =    Critic(cnn_name, output_size, (3, *input_size), dropout, bias, cnn_train).to(device)\n",
    "\n",
    "# Copiar los pesos de los críticos a los críticos objetivo\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict())\n",
    "\n",
    "# Optimización\n",
    "actor_optimizer =   optim.Adam(actor.parameters()  , lr=learning_rate)\n",
    "critic1_optimizer = optim.Adam(critic1.parameters(), lr=learning_rate)\n",
    "critic2_optimizer = optim.Adam(critic2.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if load_model and os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    actor.load_state_dict(checkpoint['actor'])\n",
    "    critic1.load_state_dict(checkpoint['critic1'])\n",
    "    critic2.load_state_dict(checkpoint['critic2'])\n",
    "    target_critic1.load_state_dict(checkpoint['target_critic1'])\n",
    "    target_critic2.load_state_dict(checkpoint['target_critic2'])\n",
    "    actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "    critic1_optimizer.load_state_dict(checkpoint['critic1_optimizer'])\n",
    "    critic2_optimizer.load_state_dict(checkpoint['critic2_optimizer'])\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "else:\n",
    "    print(\"No se cargó ningún modelo. Entrenamiento desde cero.\")\n",
    "\n",
    "# Inicializar el controlador del simulador\n",
    "if output_size == 2:\n",
    "    controller = XboxControllerEmulator()\n",
    "    print(\"Modelo de controlador cargado.\")\n",
    "else:\n",
    "    raise ValueError(\"El tamaño de salida del modelo debe ser 2 (control)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n",
      "Esperando datos de telemetría...telemetría, esperando...                                                     \n",
      "Entrenamiento interrumpido.\n",
      "Entrenamiento terminado.\n"
     ]
    }
   ],
   "source": [
    "# Iniciar el hilo de detección de teclas\n",
    "key_thread = threading.Thread(target=key_detection)\n",
    "key_thread.start()\n",
    "\n",
    "pause_event.set() # Pausar el modelo al inicio\n",
    "\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "\n",
    "episodio = 0\n",
    "sequence_buffer = []  # Buffer para almacenar las imágenes de la secuencia en caso de usar RNN o LSTM\n",
    "\n",
    "start_time = time.time()    # Tiempo de inicio del entrenamiento\n",
    "\n",
    "# Iniciar el entrenamiento\n",
    "\n",
    "try:\n",
    "    while not stop_event.is_set(): #Ciclos de episodios\n",
    "\n",
    "        if pause_event.is_set():\n",
    "            controller.reset()\n",
    "            print(\"Modelo pausado. Presione 'P' para reanudar.                                                                  \", end=\"\\r\")\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "        \n",
    "        print(f\"Episodio {episodio}\", end=\"\\r\")\n",
    "        episodio += 1\n",
    "\n",
    "        # Reiniciar el entorno\n",
    "        reset_environment()\n",
    "        episode_start_time = time.time()\n",
    "        total_reward = 0\n",
    "\n",
    "        while True: #Ciclos de pasos\n",
    "            step_start_time = time.time()\n",
    "            # Capturar la pantalla   \n",
    "            img = capture_screen(region)           \n",
    "            preprocessed_img = Image.fromarray(img.astype(np.uint8)).convert('RGB')# Convertir la imagen preprocesada a un objeto PIL y aplicar las transformaciones\n",
    "            preprocessed_img = transform(preprocessed_img)\n",
    "\n",
    "            # Telemetria del juego\n",
    "            variables = udp_listener()\n",
    "\n",
    "            if variables[\"transmitting\"] == False: # Si no se reciben datos de telemetría, continuar con el siguiente paso\n",
    "                print(\"No se están recibiendo datos de telemetría, esperando...                                                   \", end=\"\\r\")\n",
    "                continue\n",
    "\n",
    "            # Añadir la imagen preprocesada al buffer de secuencia\n",
    "            sequence_buffer.append(preprocessed_img)\n",
    "        \n",
    "            # Mantener solo las últimas imágenes en el buffer\n",
    "            if len(sequence_buffer) > seq_len:\n",
    "                sequence_buffer.pop(0)\n",
    "\n",
    "            # Verificar si tenemos suficientes imágenes para una secuencia completa\n",
    "            if len(sequence_buffer) == seq_len:\n",
    "                # Convertir la secuencia de imágenes en un tensor\n",
    "                sequence_tensor = torch.stack(sequence_buffer).unsqueeze(0).to(device)\n",
    "\n",
    "                # Habilitar precisión mixta y para usar la GPU para entrenar\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    if architecture == \"CNN\":\n",
    "                        state = sequence_tensor\n",
    "                        \"\"\" elif architecture == \"CNN_RNN\":\n",
    "                        outputs = model(sequence_tensor)\n",
    "                        state = sequence_tensor.view(-1) # Convertir la secuencia en un tensor 1D \"\"\"\n",
    "                    else:\n",
    "                        print(\"Arquitectura no soportada\")\n",
    "                        #parar la ejecucion de todo el programa\n",
    "                        raise SystemExit\n",
    "\n",
    "                    # Elegir acción basada en las características extraídas por la CNN\n",
    "                    action = actor(state)  # El actor toma el estado como entrada y produce una acción\n",
    "\n",
    "                    # Aquí deberías obtener el siguiente estado del entorno\n",
    "                    next_state = state  # En este caso, se asume que el siguiente estado es el mismo que el actual\n",
    "\n",
    "                    # Calcular la recompensa\n",
    "                    reward = calculate_reward(variables)  # Se calcula la recompensa basada en las variables del entorno\n",
    "\n",
    "                    # Verificar si el episodio ha terminado\n",
    "                    done = variables[\"tyres_out\"] == 4 or variables[\"car_damage\"] > 0  # El episodio termina si el auto está fuera de la pista o dañado\n",
    "\n",
    "                    # Enviar la acción al simulador\n",
    "                    prediction = torch.clamp(action, min=-1.0, max=1.0).tolist()[0]  # Limitar los valores de la acción entre -1.0 y 1.0 y convertir a lista\n",
    "                    controller.steering(prediction[0])  # Enviar la acción de dirección al simulador\n",
    "                    controller.throttle_break(prediction[1])  # Enviar la acción de aceleración/freno al simulador\n",
    "\n",
    "                    # Calcular la pérdida para SAC\n",
    "                    with torch.no_grad():  # Deshabilitar el cálculo de gradientes porque no necesitamos actualizar las redes objetivo\n",
    "                        next_action = actor(next_state)  # El actor toma el siguiente estado y produce la siguiente acción\n",
    "                        target_q1 = target_critic1(torch.cat([next_state, next_action], dim=-1))  # El crítico objetivo 1 calcula el valor Q para el siguiente estado y acción\n",
    "                        target_q2 = target_critic2(torch.cat([next_state, next_action], dim=-1))  # El crítico objetivo 2 calcula el valor Q para el siguiente estado y acción\n",
    "                        target_q = reward + discount_factor * torch.min(target_q1, target_q2) * (1 - done)  # Calcular el valor Q objetivo usando la recompensa y el valor Q mínimo de los críticos objetivos\n",
    "\n",
    "                    # Calcular los valores Q actuales\n",
    "                    current_q1 = critic1(torch.cat([state, action], dim=-1))  # El crítico 1 calcula el valor Q para el estado y acción actuales\n",
    "                    current_q2 = critic2(torch.cat([state, action], dim=-1))  # El crítico 2 calcula el valor Q para el estado y acción actuales\n",
    "\n",
    "                    # Calcular la pérdida de los críticos\n",
    "                    critic1_loss = F.mse_loss(current_q1, target_q)  # Calcular la pérdida del crítico 1 como el error cuadrático medio entre el valor Q actual y el objetivo\n",
    "                    critic2_loss = F.mse_loss(current_q2, target_q)  # Calcular la pérdida del crítico 2 como el error cuadrático medio entre el valor Q actual y el objetivo\n",
    "\n",
    "                    # Optimización del crítico 1\n",
    "                    critic1_optimizer.zero_grad()  # Reiniciar los gradientes del optimizador del crítico 1\n",
    "                    critic1_loss.backward()  # Calcular los gradientes de la pérdida del crítico 1\n",
    "                    critic1_optimizer.step()  # Actualizar los parámetros del crítico 1\n",
    "\n",
    "                    # Optimización del crítico 2\n",
    "                    critic2_optimizer.zero_grad()  # Reiniciar los gradientes del optimizador del crítico 2\n",
    "                    critic2_loss.backward()  # Calcular los gradientes de la pérdida del crítico 2\n",
    "                    critic2_optimizer.step()  # Actualizar los parámetros del crítico 2\n",
    "\n",
    "                    # Calcular la pérdida del actor\n",
    "                    actor_loss = -critic1(state, actor(state)).mean()  # Calcular la pérdida del actor como el negativo del valor Q promedio estimado por el crítico 1\n",
    "\n",
    "                    # Optimización del actor\n",
    "                    actor_optimizer.zero_grad()  # Reiniciar los gradientes del optimizador del actor\n",
    "                    actor_loss.backward()  # Calcular los gradientes de la pérdida del actor\n",
    "                    actor_optimizer.step()  # Actualizar los parámetros del actor\n",
    "\n",
    "                    # Actualizar las redes objetivo\n",
    "                    for target_param, param in zip(target_critic1.parameters(), critic1.parameters()):  # Para cada par de parámetros de los críticos objetivo y crítico 1\n",
    "                        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)  # Actualización suave de los parámetros del crítico objetivo 1\n",
    "\n",
    "                    for target_param, param in zip(target_critic2.parameters(), critic2.parameters()):  # Para cada par de parámetros de los críticos objetivo y crítico 2\n",
    "                        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)  # Actualización suave de los parámetros del crítico objetivo 2\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Condición para reiniciar el episodio si el auto está fuera de la pista o dañado\n",
    "            if variables[\"tyres_out\"] == 4 or variables[\"car_damage\"] > 0:\n",
    "                print(\"El auto se ha salido de la pista o ha sufrido daño, reiniciando episodio...\")\n",
    "                break  # Termina el episodio si el auto está fuera de la pista o dañado\n",
    "\n",
    "            # EN ESTA PARTE PONER UN SLEEP PARA HACER QUE LOS STEPS ESTEN ESPACIADOS DE FORMA CONSTANTE\n",
    "            time.sleep(max(0, 1/fps - (time.time() - step_start_time)))\n",
    "\n",
    "        # Guardar el modelo cada cierto número de episodios\n",
    "        if episodio % save_interval == 0:\n",
    "            save_model(episodio, model_name)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nEntrenamiento interrumpido.\")\n",
    "\n",
    "print(\"Entrenamiento terminado.\")\n",
    "\n",
    "writer.close()  # Cerrar el escritor de TensorBoard\n",
    "\n",
    "# Limpiar y cerrar\n",
    "writer.close()  # Cerrar el escritor de TensorBoard\n",
    "controller.reset()\n",
    "key_thread.join()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
